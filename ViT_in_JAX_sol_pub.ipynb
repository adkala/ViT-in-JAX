{"cells":[{"cell_type":"markdown","metadata":{"id":"FlvfeifPh9iu"},"source":["# Implementing Vision Transformers (ViT) using JAX\n","\n","### By Addison Kalanther, Anirudh Rengarajan, Jake Bringetto, and Naasir Farooqi"]},{"cell_type":"markdown","metadata":{"id":"8Mz4qJ4lkaOw"},"source":["## Introduction\n","\n","Vision Transformers (ViT) are a group of models that uses a pure transformer model to perform various vision tasks, such as image classification and object detection. In this homework, we are going to compare a classical CNN model to a ViT model and observe how ViT is able to obtain state-of-the-art results in image classification. This will be done using JAX, a machine learning framework from Google with increasing usage in research environments, and Equinox, an easy-to-use JAX library for implenting PyTorch-like neural networks.\n","\n","The objective of this homework is to allow students to become familiar with Vision Transformers, as well as their advantages and disadvantages compared to the more common CNN models. In addition, students will become familiar with JAX, better preparing them for research and academia if they wish to pursue it."]},{"cell_type":"markdown","metadata":{"id":"8n4iNvGOkNRp"},"source":["## Vision Transformers\n","\n","ViTs are a class of models using only attention to perform target tasks, without the need for convolutions, which are used to exploit spacial locality and translational invariance. Despite this, it is found that vision transformers outperform CNNs when given a large set of data. \n","\n","ViTs work by splitting an input image into fixed-size patches, linearly embedding them and then adding a positional encoding. This process is similar to how words are processed in a traditional transformer model. A diagram is added below to help visualize this process.\n","<br><br>\n","<img src=\"https://github.com/google-research/vision_transformer/blob/main/vit_figure.png?raw=true\">"]},{"cell_type":"markdown","source":["## Setup Code"],"metadata":{"id":"CdlGPnpSr_R5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBzR6WJTfTQo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670375961125,"user_tz":480,"elapsed":14878,"user":{"displayName":"Addison Kalanther","userId":"09235855594251115902"}},"outputId":"aac48075-68c9-46d9-88b5-1fc9f6cfe492"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting einops\n","  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 137 kB/s \n","\u001b[?25hCollecting equinox\n","  Downloading equinox-0.9.2-py3-none-any.whl (88 kB)\n","\u001b[K     |████████████████████████████████| 88 kB 2.9 MB/s \n","\u001b[?25hCollecting optax\n","  Downloading optax-0.1.4-py3-none-any.whl (154 kB)\n","\u001b[K     |████████████████████████████████| 154 kB 40.8 MB/s \n","\u001b[?25hCollecting jaxtyping>=0.2.5\n","  Downloading jaxtyping-0.2.8-py3-none-any.whl (17 kB)\n","Requirement already satisfied: jax>=0.3.4 in /usr/local/lib/python3.8/dist-packages (from equinox) (0.3.25)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (3.3.0)\n","Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (1.7.3)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from jax>=0.3.4->equinox) (4.1.1)\n","Collecting typeguard>=2.13.3\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax) (1.3.0)\n","Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax) (0.3.25+cuda11.cudnn805)\n","Collecting chex>=0.1.5\n","  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n","\u001b[K     |████████████████████████████████| 85 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n","Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.1.7)\n","Installing collected packages: typeguard, jaxtyping, chex, optax, equinox, einops\n","  Attempting uninstall: typeguard\n","    Found existing installation: typeguard 2.7.1\n","    Uninstalling typeguard-2.7.1:\n","      Successfully uninstalled typeguard-2.7.1\n","Successfully installed chex-0.1.5 einops-0.6.0 equinox-0.9.2 jaxtyping-0.2.8 optax-0.1.4 typeguard-2.13.3\n"]}],"source":["%pip install einops equinox optax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QMLrTZa92g0","colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["9a18a5c9d74b48d182177124c53a40cc","f73fdfdf239a4efdb3196c16f6919f67","f9e4a656cd764e56b09a77f8f36035e4","35d716f21a584f80b01bf6ce35cf8218","9252c98b4fe640aead8221c6de5ec6cd","4622e74f90274815999288e3310ac4c6","8d7b84120aee450e9a3d97ee2de9a367","35a4dfa3238745acb8057471fee63cb7","f76177cc08154a2c877a9a871ba0b31b","8109ca50ce2d4b4ba07335850faef47d","763bbd7bf6064130825ebe1c9c90ae66"]},"executionInfo":{"status":"ok","timestamp":1670376003908,"user_tz":480,"elapsed":41103,"user":{"displayName":"Addison Kalanther","userId":"09235855594251115902"}},"outputId":"9de44ade-7615-4e1b-8dd3-8949ef38e9b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a18a5c9d74b48d182177124c53a40cc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./cifar-10-python.tar.gz to ./\n","Files already downloaded and verified\n"]}],"source":["### Import necessary libraries and prepare dataset ###\n","\n","import torch\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import einops\n","\n","from PIL import Image\n","from torch import nn, Tensor, optim\n","from einops.layers.torch import Rearrange, Reduce\n","\n","import equinox as eqx\n","import equinox.nn as enn\n","\n","import jax\n","import jax.nn as jnn\n","import jax.numpy as jnp\n","import optax\n","\n","from typing import Optional, Any\n","\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","training_set = torchvision.datasets.CIFAR10('./', train=True, download=True, transform=transform)\n","training_dataloader = torch.utils.data.DataLoader(training_set, batch_size=128, shuffle=True, num_workers=2)\n","\n","test_set = torchvision.datasets.CIFAR10('./', train=False, download=True, transform=transform)\n","test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"Jl5yLFM99Hta"},"source":["## Creating a ViT via PyTorch\n","\n","Before implementing a ViT using JAX, students will become familiar with ViTs by implementing one via the familiar PyTorch framework and einops, a framework for easier tensor manipulation.\n","\n","To compare performance and to be used as reference for later parts of this homework, we have implemented LeNet-5 in PyTorch for you. "]},{"cell_type":"code","source":["### LeNet via PyTorch ###\n","\n","class LeNet(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.conv1 = nn.Conv2d(3, 6, 5)\n","    self.pool = nn.MaxPool2d(2, 2)\n","    self.conv2 = nn.Conv2d(6, 16, 5)\n","    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","    self.fc2 = nn.Linear(120, 84)\n","    self.fc3 = nn.Linear(84, 10)\n","\n","  def forward(self, x: Tensor) -> Tensor:\n","    x = self.pool(F.relu(self.conv1(x)))\n","    x = self.pool(F.relu(self.conv2(x)))\n","    x = x.view(-1, 16  * 5 * 5) #flatten\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = F.softmax(self.fc3(x), dim=1)\n","    return x\n","\n","def test_pt(model):\n","  correct = 0\n","  total = 0\n","\n","  model.eval()\n","  for data in test_dataloader:\n","    inputs, labels = data\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","\n","    outputs = model(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","\n","  return correct / total\n","\n","def train_pt(model, learning_rate, momentum, epochs, verbose=False):\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n","\n","  for epoch in range(epochs):\n","    for i, data in enumerate(training_dataloader, 0):\n","      inputs, labels = data\n","      inputs = inputs.to(device)\n","      labels = labels.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      if verbose and i % 20 == 0:\n","         print('Batch: {}, Loss: {:.3f}'.format(i, loss))\n","\n","    acc = test_pt(model)\n","    print('Epoch: {}, Loss: {:.3f}, Test Accuracy: {:.3f}'.format(epoch, loss, acc))\n","  return acc\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"qwUy5HLlzws1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TODO: Tune hyperparameters such that LeNet-5 achieves an accuracy of over 45% ###\n","learning_rate_lenet = 0.02 \n","momentum_lenet = 0.96\n","epochs_lenet = 5\n","\n","lenet = LeNet().to(torch.device(device))\n","train_pt(lenet, learning_rate_lenet, momentum_lenet, epochs_lenet)\n","print('Final Accuracy: {:.3f}'.format(test_pt(lenet)))"],"metadata":{"id":"I7UrgxWS2ja6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670358327338,"user_tz":480,"elapsed":66564,"user":{"displayName":"Anirudh Rengarajan","userId":"17609811849278760213"}},"outputId":"a475be1a-7ac2-456e-ee7b-39a67480a038"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, Loss: 2.195, Test Accuracy: 0.242\n","Epoch: 1, Loss: 2.105, Test Accuracy: 0.337\n","Epoch: 2, Loss: 2.061, Test Accuracy: 0.409\n","Epoch: 3, Loss: 2.037, Test Accuracy: 0.439\n","Epoch: 4, Loss: 2.027, Test Accuracy: 0.455\n","Final Accuracy: 0.455\n"]}]},{"cell_type":"code","source":["### TODO: Implement a ViT in PyTorch ###\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self, emb_size=768, num_heads=8):\n","    super().__init__()\n","    self.emb_size = emb_size\n","    self.num_heads = num_heads\n","\n","    self.qkv = nn.Linear(emb_size, emb_size * 3)\n","    self.fc1 = nn.Linear(emb_size, emb_size)\n","\n","  def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n","    qkv = einops.rearrange(self.qkv(x), 'b n (h d qkv) -> (qkv) b h n d', h = self.num_heads, qkv = 3)\n","    queries, keys, values = qkv[0], qkv[1], qkv[2]\n","    \n","    dot_product = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n","    if mask is not None:\n","      fill_value = torch.finfo(torch.float32).min\n","      dot_product.mask_fill(~mask, fill_value)\n","\n","    scaling = self.emb_size ** (1/2)\n","    att = F.softmax(dot_product, dim=-1) / scaling\n","    out = torch.einsum('bhal, bhlv -> bhav', att, values)\n","    out = einops.rearrange(out, \"b h n d -> b n (h d)\")\n","    out = self.fc1(out)\n","    return out\n","\n","\n","class Embedding(nn.Module):\n","  def __init__(self, patch_size=16, in_channels=3, emb_size=768, img_size=224):\n","    self.patch_size = patch_size\n","    super().__init__()\n","\n","    # Patch Embedding\n","    self.patch_embedding = nn.Sequential(\n","        Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size, s2=patch_size),\n","        nn.Linear(patch_size * patch_size * in_channels, emb_size)\n","    )\n","\n","    # CLS Token\n","    self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n","\n","    # Position Embedding\n","    self.positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, emb_size))\n","\n","  def forward(self, x: Tensor) -> Tensor:\n","    # Patch embedding\n","    x = self.patch_embedding(x)\n","    # Prepend CLS token (Hint: Einops may be useful here)\n","    x = torch.cat([einops.repeat(self.cls_token, '() n e -> b n e', b=x.shape[0]), x], dim=1)\n","    # Add position embeddings\n","    x += self.positions\n","\n","    return x\n","\n","class Encoder(nn.Module):\n","  def __init__(self, emb_size=768, mlp_exp=4):\n","    super().__init__()\n","\n","    # Layernorm\n","    self.norm = nn.LayerNorm(emb_size)\n","\n","    # Multi-head Attention\n","    self.mha = MultiHeadAttention(emb_size)\n","\n","    # MLP (Hint: Use Sequential Layers with a GELU)\n","    self.mlp = nn.Sequential(\n","        nn.Linear(emb_size, mlp_exp * emb_size),\n","        nn.GELU(), \n","        nn.Linear(mlp_exp * emb_size, emb_size) \n","    )\n","    \n","\n","  def forward(self, x: Tensor) -> Tensor:\n","    # Residual block 1\n","    res = x\n","    # Apply Layernorm\n","    x = self.norm(x) \n","    # Apply Multi-head attention\n","    x = self.mha(x)\n","    # Add residual\n","    x = res + x \n","\n","    # Residual block 2\n","    res = x \n","    # Apply Layernorm\n","    x = self.norm(x) \n","    # Apply MLP\n","    x = self.mlp(x)\n","    # Add residual\n","    x = res + x \n","\n","    return x\n","\n","class ClassificationHead(nn.Sequential):\n","  def __init__(self, emb_size=768, n_classes = 10):\n","    super().__init__(\n","        Reduce('b n e -> b e', reduction='mean'),\n","        nn.LayerNorm(emb_size),\n","        nn.Linear(emb_size, n_classes)\n","    )\n","\n","class ViT(nn.Sequential):\n","  def __init__(self, img_size=32, in_channels=3, n_classes=10, patch_size=16, emb_size=768, depth=1, enc_mlp_exp=4, **kwargs):\n","    super().__init__(\n","        Embedding(patch_size, in_channels, emb_size, img_size), \n","        *[Encoder(emb_size, enc_mlp_exp) for _ in range(depth)],\n","        ClassificationHead(emb_size, n_classes)\n","    )\n"],"metadata":{"id":"NmIoZSn8CCYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate_vit = 0.01\n","momentum_vit = 0.9 \n","epochs_vit = 5\n","encoder_depth = 5\n","\n","vit = ViT(depth=encoder_depth).to(torch.device(device))\n","acc = train_pt(vit, learning_rate_vit, momentum_vit, epochs_vit, verbose=False)"],"metadata":{"id":"lla5Z8baAOvb","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"status":"error","timestamp":1670376016241,"user_tz":480,"elapsed":6838,"user":{"displayName":"Addison Kalanther","userId":"09235855594251115902"}},"outputId":"1632e72f-59c8-4dc3-a624-3f3d17b23c3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 768])\n","torch.Size([1, 1, 768])\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-1e4266cfbb3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_pt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_vit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum_vit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_vit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-306dab6539b7>\u001b[0m in \u001b[0;36mtrain_pt\u001b[0;34m(model, learning_rate, momentum, epochs, verbose)\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"0eVfLc8j2M5M"},"source":["## Getting familiar with JAX by writing a CNN\n","\n","\n","Before implementing a ViT in JAX, we will get familiar with the framework by first implementing a CNN. To become familiar with JAX and use as a resource throughout this notebook, please look at this JAX 101 tutorial: https://jax.readthedocs.io/en/latest/jax-101/index.html\n","<br><br>\n","#### Quick TLDR\n","JAX is an efficient just-in-time compiler for linear algebra operations. It allows for NumPy code to run not only on the CPU but also the GPU and TPU as well. JAX can be used as a drop-in replacement for NumPy due to Python's implementation of duck-typing.\n","<br><br>\n","For practice, we will implement LeNet-5 in JAX via Equinox. "]},{"cell_type":"code","source":["### Implementing LeNet in JAX via Equinox ###\n","\n","class LeNetEqx(eqx.Module):\n","  features: eqx.Module\n","  classifier: eqx.Module\n","\n","  def __init__(self, key: Optional[\"jax.random.PRNGKey\"] = None):\n","    super().__init__()\n","\n","    if key is None:\n","      key = jax.random.PRNGKey(0)\n","    keys = jax.random.split(key, 5)\n","\n","    self.features = enn.Sequential([\n","        enn.Conv2d(3, 6, kernel_size=5, key=keys[0]),\n","        enn.Lambda(jnn.relu),\n","        enn.MaxPool2d(kernel_size=2, stride=2),\n","        enn.Conv2d(6, 16, kernel_size=5, key=keys[1]),\n","        enn.Lambda(jnn.relu),\n","        enn.MaxPool2d(kernel_size=2, stride=2),\n","    ])\n","\n","    self.classifier = enn.Sequential([\n","        enn.Linear(16 * 5 * 5, 120, key=keys[2]),\n","        enn.Lambda(jnn.relu),\n","        enn.Linear(120, 84, key=keys[3]),\n","        enn.Lambda(jnn.relu),\n","        enn.Linear(84, 10, key=keys[4]),\n","        \n","    ])\n","\n","  def __call__(self, x: jax.Array) -> jax.Array:\n","    x = self.features(x)\n","    #x = x.view(-1, 16 * 5 * 5) #flatten\n","    x = jnp.ravel(x)\n","    x = self.classifier(x)\n","    x = jnn.softmax(x)\n","    return x\n","\n","  def __iter__(self):\n","    for attr, value in self.__dict__.items():\n","      yield attr, value"],"metadata":{"id":"RHV_Luz99RZ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Building training and testing functions ###\n","\n","def train_eqx(model, learning_rate, momentum, epochs, verbose=False):\n","  optimizer = optax.sgd(learning_rate, momentum=momentum)\n","  opt_state = optimizer.init(eqx.filter(model, eqx.is_array))\n","\n","  get_acc = jax.jit(test_eqx)\n","\n","  @eqx.filter_value_and_grad # allows for (original value, gradient of function) to be returned\n","  def compute_loss(model, x, y):\n","    pred_y = jax.vmap(model)(x)\n","    y = jnn.one_hot(y, 10) \n","    softmax_ce = optax.softmax_cross_entropy(pred_y, y) \n","    return jnp.mean(softmax_ce) \n","\n","  @eqx.filter_jit # allows for Just-in-Time compilation of Python function\n","  def make_step(model, x, y, opt_state): \n","    loss, grads = compute_loss(model, x, y)\n","    updates, opt_state = optimizer.update(grads, opt_state)\n","    model = eqx.apply_updates(model, updates)\n","    return model, opt_state, loss\n","\n","  for epoch in range(epochs):\n","    for i, data in enumerate(training_dataloader, 0):\n","      inputs, labels = data\n","\n","      inputs = jnp.array(inputs)\n","      labels = jnp.array(labels)\n","\n","      model, opt_state, loss = make_step(model, inputs, labels, opt_state)\n","\n","      if verbose and i % 20 == 0:\n","         print('Batch: {}, Loss: {:.3f}'.format(i, loss))\n","\n","    acc = test_eqx(model)\n","    print('Epoch: {}, Loss: {:.3f}, Test Accuracy: {:.3f}'.format(epoch, loss, acc))\n","\n","def test_eqx(model):\n","  correct = 0\n","  total = 0\n","\n","  @eqx.filter_jit\n","  def get_correct_total(model, inputs, outputs):\n","    outputs = jax.vmap(model)(inputs)\n","    predicted = jnp.argmax(outputs, axis=1)\n","    total = labels.shape[0]\n","    correct = (predicted == labels).sum()\n","    return correct, total\n","\n","  for data in test_dataloader:\n","    inputs, labels = data\n","\n","    inputs = jnp.array(inputs)\n","    labels = jnp.array(labels)\n","\n","    c, t = get_correct_total(model, inputs, labels)\n","    correct += c\n","    total += t\n","\n","  return correct / total"],"metadata":{"id":"6MpMz2YFCL9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate_ln_eqx = 0.05\n","momentum_ln_eqx = 0.9\n","epochs_ln_eqx = 5\n","\n","seed = 123\n","rng = jax.random.PRNGKey(seed)\n","\n","lenet_eqx = LeNetEqx(rng)\n","train_eqx(lenet_eqx, learning_rate_ln_eqx, momentum_ln_eqx, epochs_ln_eqx)"],"metadata":{"id":"seIrjqftFyyG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating a ViT using JAX\n","\n","Now that we have gotten familiar with JAX by writing a model using CNN layers, we will transition into creating a ViT. Where you can, try to utilize the just-in-time (jit) functionality of JAX to speed up training and prediction where possible.\n","<br><br>\n"],"metadata":{"id":"H-SV3Uq5LH8w"}},{"cell_type":"code","source":["### Helper classes (feel free to use or not use them) ###\n","\n","class RearrangeEqx(eqx.Module):\n","    \"\"\"\n","    Equinox Module to act as a Rearrange layer (from einops)\n","    \"\"\"\n","\n","    pattern: str\n","    kwargs: dict\n","\n","    def __init__(self, pattern, **kwargs):\n","      self.pattern = pattern\n","      self.kwargs = kwargs\n","\n","    def __call__(self, input, **kwargs):\n","        return einops.rearrange(input, self.pattern, **self.kwargs)\n","\n","class ReduceEqx(eqx.Module):\n","    \"\"\"\n","    Equinox Module to act as a Reduce layer (from einops)\n","    \"\"\"\n","\n","    pattern: str\n","    kwargs: dict\n","\n","    def __init__(self, pattern, **kwargs):\n","      self.pattern = pattern\n","      self.kwargs = kwargs\n","\n","    def __call__(self, input, **kwargs):\n","        return einops.reduce(input, self.pattern, **self.kwargs)"],"metadata":{"id":"h3hY3V3aLJUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttentionEqx(eqx.Module):\n","  emb_size: int\n","  num_heads: int\n","  qkv: eqx.Module\n","  fc: eqx.Module\n","\n","  def __init__(self, emb_size=768, num_heads=8, key: Optional[\"jax.random.PRNGKey\"] = None):\n","    super().__init__()\n","    self.emb_size = emb_size\n","    self.num_heads = num_heads\n","\n","    if key is None:\n","      key = jax.random.PRNGKey(0)\n","    keys = jax.random.split(key, 2)\n","\n","    self.qkv = enn.Linear(emb_size, emb_size * 3, key=keys[0])\n","    self.fc = enn.Linear(emb_size, emb_size, key=keys[1])\n","\n","  def __call__(self, x: jax.Array, mask: jax.Array = None, **kwargs) -> jax.Array:\n","    qkv = einops.rearrange(jax.vmap(self.qkv)(x), 'n (h d qkv) -> (qkv) h n d', h = self.num_heads, qkv = 3)\n","    queries, keys, values = qkv[0], qkv[1], qkv[2]\n","\n","    dot_product = jnp.einsum('hqd, hkd -> hqk', queries, keys)\n","    if mask is not None:\n","      fill_value = jnp.finfo(jnp.float32).min\n","      dot_product = jnp.where(mask, fill_value, dot_product)\n","\n","    scaling = self.emb_size ** (1/2)\n","    att = jnn.softmax(dot_product, axis=-1) / scaling\n","    out = jnp.einsum('hal, hlv -> hav', att, values)\n","    out = einops.rearrange(out, 'h n d -> n (h d)')\n","    out = jax.vmap(self.fc)(out)\n","    return out\n","\n","class EmbeddingEqx(eqx.Module):\n","  patch_size: int\n","  rearrange: eqx.Module\n","  embedding_mlp: eqx.Module\n","  cls_token: jnp.ndarray\n","  positions: jnp.ndarray\n","\n","  def __init__(self, patch_size=16, in_channels=3, emb_size=768, img_size=224, key: Optional[\"jax.random.PRNGKey\"] = None):\n","    self.patch_size = patch_size\n","    super().__init__()\n","\n","    if key is None:\n","      key = jax.random.PRNGKey(0)\n","    keys = jax.random.split(key, 3)\n","\n","    # Patch Embedding\n","    self.rearrange = RearrangeEqx('c (h s1) (w s2) -> (h w) (s1 s2 c)', s1=patch_size, s2=patch_size)\n","    self.embedding_mlp = enn.Linear(patch_size * patch_size * in_channels, emb_size, key=keys[0])\n","\n","    # CLS Token\n","    self.cls_token = jax.random.uniform(keys[1], shape=(1, emb_size))\n","\n","    # Position Embedding\n","    self.positions = jax.random.uniform(keys[2], shape=((img_size // patch_size) ** 2 + 1, emb_size))\n","\n","  def __call__(self, x: jax.Array, **kwargs) -> jax.Array:\n","    x = self.rearrange(x)\n","    x = jax.vmap(self.embedding_mlp)(x)\n","    x = jnp.concatenate([self.cls_token, x], axis=0)\n","    x += self.positions\n","\n","    return x\n","\n","class EncoderEqx(eqx.Module):\n","  mha: eqx.Module\n","  mlp: eqx.Module\n","  norm: eqx.Module\n","\n","  def __init__(self, emb_size=768, mlp_exp=4, key: Optional[\"jax.random.PRNGKey\"] = None):\n","    super().__init__()\n","\n","    if key is None:\n","      key = jax.random.PRNGKey(0)\n","    keys = jax.random.split(key, 3)\n","\n","    # LayerNorm\n","    self.norm = enn.LayerNorm(emb_size)\n","\n","    # Multi-head Attention\n","    self.mha = MultiHeadAttentionEqx(emb_size, key=keys[0])\n","\n","    # MLP\n","    self.mlp = enn.Sequential([\n","        enn.Linear(emb_size, mlp_exp * emb_size, key=keys[1]),\n","        enn.Lambda(jnn.gelu),\n","        enn.Linear(mlp_exp * emb_size, emb_size, key=keys[2])\n","    ])\n","\n","\n","  def __call__(self, x: jax.Array, **kwargs) -> jax.Array:\n","    # Residual block 1\n","    res = x\n","    # Apply Layernorm\n","    x = self.norm(x) \n","    # Apply Multi-head attention\n","    x = self.mha(x)\n","    # Add residual\n","    x = res + x \n","\n","\n","    # Residual block 2\n","    res = x \n","    # Apply Layernorm\n","    x = self.norm(x) \n","    # Apply MLP\n","    x = jax.vmap(self.mlp)(x)\n","    # Add residual\n","    x = res + x \n","\n","    return x\n","\n","class ClassificationHeadEqx(enn.Sequential):\n","  def __init__(self, emb_size=768, n_classes=10, key: Optional[\"jax.random.PRNGKey\"] = None):\n","    if key is None:\n","      key = jax.random.PRNGKey(0)\n","\n","    super().__init__([\n","        ReduceEqx('n e -> e', reduction='mean'),\n","        enn.LayerNorm(emb_size),\n","        enn.Linear(emb_size, n_classes, key=key)\n","    ])\n","\n","class ViTEqx(enn.Sequential):\n","  def __init__(self, img_size=32, in_channels=3, n_classes=10, patch_size=16, emb_size=768, depth=1, enc_mlp_exp=4, key: Optional[\"jax.random.PRNGKey\"] = None, **kwargs):\n","    if key is None:\n","      key = jax.random.PRNGKey(0)\n","    keys = jax.random.split(key, 2 + depth)\n","\n","    super().__init__(\n","        [EmbeddingEqx(patch_size, in_channels, emb_size, img_size, key=keys[0]), \n","        *[EncoderEqx(emb_size, enc_mlp_exp, key=keys[i + 1]) for i in range(depth)],\n","        ClassificationHeadEqx(emb_size, n_classes, key=keys[depth + 1])]\n","    )"],"metadata":{"id":"52N7G-lbLLU7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate_vit_eqx = 0.01 # [0.005, 0.01, 0.03, 0.05] BEST LR=0.01\n","momentum_vit_eqx = 0.9 # [0.8, 0.9, 0.95, 0.99] BEST Momentum=0.9\n","epochs_vit_eqx = 5\n","encoder_depth_eqx = 5\n","\n","seed = 456 # edit the seed to change pseudo-random behavior\n","rng = jax.random.PRNGKey(seed)\n","\n","vit_eqx = ViTEqx(key=rng)\n","acc = train_eqx(vit_eqx, learning_rate_vit_eqx, momentum_vit_eqx, epochs_vit_eqx, verbose=True)\n","print(acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":468},"id":"O4-vE2QcLNUL","executionInfo":{"status":"error","timestamp":1670376745564,"user_tz":480,"elapsed":85529,"user":{"displayName":"Addison Kalanther","userId":"09235855594251115902"}},"outputId":"3caa3175-f8c1-469e-df89-8730c87d8de4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch: 0, Loss: 2.432\n","Batch: 20, Loss: 2.289\n","Batch: 40, Loss: 2.109\n","Batch: 60, Loss: 1.911\n","Batch: 80, Loss: 2.162\n","Batch: 100, Loss: 2.005\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-ab64cba1192b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvit_eqx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mViTEqx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eqx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit_eqx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_vit_eqx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum_vit_eqx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_vit_eqx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-a92cd015a972>\u001b[0m in \u001b[0;36mtrain_eqx\u001b[0;34m(model, learning_rate, momentum, epochs, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/jit.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(_JitWrapper__self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fun_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/jit.py\u001b[0m in \u001b[0;36m_fun_wrapper\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mdynamic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/equinox/module.py\u001b[0m in \u001b[0;36mtree_unflatten\u001b[0;34m(cls, aux, dynamic_field_values)\u001b[0m\n\u001b[1;32m    260\u001b[0m         )\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_field_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"12R6g6jI-BjG7AIFq3gWg4_tvJOuDXo5I","timestamp":1672628805107}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9a18a5c9d74b48d182177124c53a40cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f73fdfdf239a4efdb3196c16f6919f67","IPY_MODEL_f9e4a656cd764e56b09a77f8f36035e4","IPY_MODEL_35d716f21a584f80b01bf6ce35cf8218"],"layout":"IPY_MODEL_9252c98b4fe640aead8221c6de5ec6cd"}},"f73fdfdf239a4efdb3196c16f6919f67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4622e74f90274815999288e3310ac4c6","placeholder":"​","style":"IPY_MODEL_8d7b84120aee450e9a3d97ee2de9a367","value":"100%"}},"f9e4a656cd764e56b09a77f8f36035e4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_35a4dfa3238745acb8057471fee63cb7","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f76177cc08154a2c877a9a871ba0b31b","value":170498071}},"35d716f21a584f80b01bf6ce35cf8218":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8109ca50ce2d4b4ba07335850faef47d","placeholder":"​","style":"IPY_MODEL_763bbd7bf6064130825ebe1c9c90ae66","value":" 170498071/170498071 [00:32&lt;00:00, 3177552.75it/s]"}},"9252c98b4fe640aead8221c6de5ec6cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4622e74f90274815999288e3310ac4c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d7b84120aee450e9a3d97ee2de9a367":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35a4dfa3238745acb8057471fee63cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f76177cc08154a2c877a9a871ba0b31b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8109ca50ce2d4b4ba07335850faef47d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"763bbd7bf6064130825ebe1c9c90ae66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}